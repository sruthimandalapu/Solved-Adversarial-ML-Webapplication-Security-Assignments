{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a proper tensorflow version Our model was build on TF2.14 and it does cause some compatibility issues with TF2.17 so make sure you use the right version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_privacy\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "import datetime\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount google drive on your runtime using and authorization code.\n",
    "# more details here: https://colab.research.google.com/notebooks/io.ipynb\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Path to the directory containing the project files (CHANGE THIS PATH TO THE DIRECTORY ON YOUR COMPUTER)\n",
    "PROJECT_ROOT_DIR = os.getcwd() + '/'\n",
    "\n",
    "# Path to the directory containing the dataset relative to project file\n",
    "DATA_DIR = 'drive/MyDrive/CS5331_CS4331/HW1/GTSRB_dataset/'\n",
    "\n",
    "#path to the directory you want to use for saving models relative to the project file\n",
    "MODEL_DIR = 'drive/MyDrive/CS5331_CS4331/HW1/GTSRB_dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciton for loading the dataset\n",
    "# Code from advml-traffic-sign (https://github.com/inspire-group/advml-traffic-sign)\n",
    "def load_dataset_GTSRB(n_channel=3, train_file_name=None):\n",
    "    \"\"\"\n",
    "    Load GTSRB data as a (datasize) x (channels) x (height) x (width) numpy\n",
    "    matrix. Each pixel is rescaled to the range [0,1].\n",
    "    \"\"\"\n",
    "\n",
    "    def load_pickled_data(file, columns):\n",
    "        \"\"\"\n",
    "        Loads pickled training and test data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file    : string\n",
    "                          Name of the pickle file.\n",
    "        columns : list of strings\n",
    "                          List of columns in pickled data we're interested in.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        A tuple of datasets for given columns.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(file, mode='rb') as f:\n",
    "            dataset = pickle.load(f)\n",
    "        return tuple(map(lambda c: dataset[c], columns))\n",
    "\n",
    "    def preprocess(x, n_channel):\n",
    "        \"\"\"\n",
    "        Preprocess dataset: turn images into grayscale if specified, normalize\n",
    "        input space to [0,1], reshape array to appropriate shape for NN model\n",
    "        \"\"\"\n",
    "\n",
    "        if n_channel == 3:\n",
    "            # Scale features to be in [0, 1]\n",
    "            x = (x / 255.).astype(np.float32)\n",
    "        else:\n",
    "            # Convert to grayscale, e.g. single Y channel\n",
    "            x = 0.299 * x[:, :, :, 0] + 0.587 * x[:, :, :, 1] + \\\n",
    "                0.114 * x[:, :, :, 2]\n",
    "            # Scale features to be in [0, 1]\n",
    "            x = (x / 255.).astype(np.float32)\n",
    "            x = x[:, :, :, np.newaxis]\n",
    "        return x\n",
    "\n",
    "    # Load pickle dataset\n",
    "    if train_file_name is None:\n",
    "        x_train, y_train = load_pickled_data(\n",
    "            PROJECT_ROOT_DIR + DATA_DIR + 'train.p', ['features', 'labels'])\n",
    "    else:\n",
    "        x_train, y_train = load_pickled_data(\n",
    "            PROJECT_ROOT_DIR + DATA_DIR + train_file_name, ['features', 'labels'])\n",
    "    x_val, y_val = load_pickled_data(\n",
    "        PROJECT_ROOT_DIR + DATA_DIR + 'valid.p', ['features', 'labels'])\n",
    "    x_test, y_test = load_pickled_data(\n",
    "        PROJECT_ROOT_DIR + DATA_DIR + 'test.p', ['features', 'labels'])\n",
    "\n",
    "    # Preprocess loaded data\n",
    "    x_train = preprocess(x_train, n_channel)\n",
    "    x_val = preprocess(x_val, n_channel)\n",
    "    x_test = preprocess(x_test, n_channel)\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "# Load the images and labels. These images are RGB so we have 3 channels\n",
    "imgs_train, labels_train, imgs_val, labels_val, imgs_test, labels_test = load_dataset_GTSRB(n_channel=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_train=imgs_train[0:34688,:,:,:].copy()\n",
    "labels_train=labels_train[0:34688].copy()\n",
    "imgs_val=imgs_val[0:4352,:,:,:].copy()\n",
    "labels_val=labels_val[0:4352].copy()\n",
    "imgs_test=imgs_test[0:12544,:,:,:].copy()\n",
    "labels_test=labels_test[0:12544].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some constants that you might need it is not neccessary that you will use all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants (GTSRB)\n",
    "NUM_LABELS = 43                             # Number of labels or classes for classification\n",
    "BATCH_SIZE = 128                            # Size of batch\n",
    "HEIGHT = 32                                 # Height of input image\n",
    "WIDTH = 32                                  # Width of input image\n",
    "N_CHANNEL = 3                               # Number of channels\n",
    "OUTPUT_DIM = 43                             # Number of output dimension\n",
    "\n",
    "# Set training hyperparameters\n",
    "NUM_EPOCH = 14                             # Number of epoch to train\n",
    "LR = 0.01                                 # Learning rate\n",
    "RBW = True #restore best weights\n",
    "PATIENCE = 3# how many epochs between improvements\n",
    "\n",
    "INPUT_SHAPE = (HEIGHT, WIDTH, N_CHANNEL)  # Input shape of model\n",
    "IMG_SHAPE = (HEIGHT, WIDTH, N_CHANNEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the labels to one-hot encoding (to input to the models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up labels\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "labels_train_cat = to_categorical(labels_train, NUM_LABELS)\n",
    "labels_test_cat = to_categorical(labels_test, NUM_LABELS)\n",
    "labels_val_cat = to_categorical(labels_val, NUM_LABELS)\n",
    "\n",
    "\n",
    "print('Labels train shape: {}'.format(labels_train.shape))\n",
    "print('Labels train catagorical shape: {}\\n'.format(labels_train_cat.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install tensorflow privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install tensorflow privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some import you might need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_privacy\n",
    "\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers.legacy import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DP-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#create our model\n",
    "def build_model():\n",
    "    #create VGG16 model with properties we want.\n",
    "    base_model = tf.keras.applications.VGG16(include_top=False,\n",
    "                                    weights=\"imagenet\",\n",
    "                                    input_shape=INPUT_SHAPE)\n",
    "\n",
    "    #create fully connected layers\n",
    "    #by not including the top we need to create these layers ourselves\n",
    "    #input and output layers\n",
    "    # Add a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    # Add a fully-connected layer\n",
    "    x = Dense(2048, activation='relu')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    # Add a softmax layer with 43 classes\n",
    "    predictions = Dense(OUTPUT_DIM, activation='softmax', name ='softmax')(x)\n",
    "\n",
    "    # The model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "      # Use TensorFlow Privacy's optimizer\n",
    "    optimizer = tensorflow_privacy.DPKerasSGDOptimizer(\n",
    "        l2_norm_clip=2,\n",
    "        num_microbatches=128,\n",
    "        noise_multiplier=1.6,\n",
    "        learning_rate=LR\n",
    "    )\n",
    "\n",
    "    # Set the loss function, use from_logits=False because softmax is applied\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=False,  # Set this to False\n",
    "        reduction=tf.losses.Reduction.NONE\n",
    "    )\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "t = datetime.datetime.now()\n",
    "\n",
    "#fit the model\n",
    "history = model.fit(x=imgs_train, y=labels_train_cat,\n",
    "          epochs=NUM_EPOCH,\n",
    "          validation_data=(imgs_val, labels_val_cat),\n",
    "          batch_size=BATCH_SIZE)\n",
    "\n",
    "print('Training time: %s\\n' % (datetime.datetime.now() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on train,validation,test images\n",
    "t = datetime.datetime.now()\n",
    "evals_test = model.evaluate(imgs_test, labels_test_cat)\n",
    "print(\"Classification Accuracy Test: \", evals_test[1])\n",
    "print('Inference time: %s \\n' % (datetime.datetime.now() - t))\n",
    "\n",
    "t = datetime.datetime.now()\n",
    "evals_test = model.evaluate(imgs_val, labels_val_cat)\n",
    "print(\"Classification Accuracy Validation: \", evals_test[1])\n",
    "print('Inference time: %s \\n' % (datetime.datetime.now() - t))\n",
    "\n",
    "t = datetime.datetime.now()\n",
    "evals_test = model.evaluate(imgs_train, labels_train_cat)\n",
    "print(\"Classification Accuracy Train: \", evals_test[1])\n",
    "print('Inference time: %s' % (datetime.datetime.now() - t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model architecture to a JSON file\n",
    "import json\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model_architecture8.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save model weights to an .h5 file\n",
    "model.save_weights(\"model_weights8.h5\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
